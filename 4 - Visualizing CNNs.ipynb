{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 - Visualizing CNNs.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zhvgR08m7Rkv"},"source":["# Hands-on Session 4: Visualizing CNNs"]},{"cell_type":"markdown","metadata":{"id":"W62jhybFOHTB"},"source":["Deep learning neural networks can make useful and skillful predictions but it is not clear how or why a given prediction was made.\n","\n","Convolutional neural networks, have internal structures that are designed to operate upon two-dimensional image data, and as such preserve the spatial relationships for what was learned by the model. Specifically, the two-dimensional filters learned by the model can be inspected and visualized to discover the types of features that the model will detect, and the feature maps in the convolutional layers can be inspected to understand exactly what features were detected for a given input image.\n","\n","We can create simple visualizations for filters and feature maps in a convolutional neural network. In this hands-on session, you will learn the following:\n","\n","*   How to develop a visualization for specific filters in a convolutional neural network.\n","*   How to develop a visualization for specific feature maps in a convolutional neural network.\n","\n"]},{"cell_type":"code","metadata":{"id":"SnQh1Gr87dTS"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8GQB3jn7zTX"},"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-J0QySJmwZq_"},"source":["### How to Visualize Filters and Feature Maps\n","The activation maps, called feature maps, capture the result of applying the filters to input, such as the input image or another feature map. The idea of visualizing a feature map for a specific input image would be to understand what features of the input are detected or preserved in the feature maps. \n","\n","In order to explore the visualization of feature maps, we need input for the VGG16 model that can be used to create activations. We will use a simple photograph of a bird. Specifically, a Robin, taken by Chris Heald (https://www.flickr.com/photos/husker_alum/8628754308/) and released under a permissive license.\n","<br><br>\n","\n","**I. Display the information of the feature maps output by each CNN Layer**\n","<br>\n","Next, we need a clearer idea of the shape of the feature maps output by each of the convolutional layers and the layer index number so that we can retrieve the appropriate layer output. The example below will enumerate all layers in the model and print the output size or feature map size for each convolutional layer as well as the layer index in the model."]},{"cell_type":"code","metadata":{"id":"kX_BgBas7RwE"},"source":["# summarize filters in each convolutional layer\n","from keras.applications.vgg16 import VGG16\n","from matplotlib import pyplot\n","# load the model\n","model = VGG16()\n","# summarize filter shapes\n","for layer in model.layers:\n","\t# check for convolutional layer\n","\tif 'conv' not in layer.name:\n","\t\tcontinue\n","\t# get filter weights\n","\tfilters, biases = layer.get_weights()\n","\tprint(layer.name, filters.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bj-AL9HHvIuq"},"source":["**II. Visualization of filters learnt in a CNN**\n","\n","The code example below produces the visualization of the filters for the last convolutional layer in the VGG16 model for a bird input image."]},{"cell_type":"code","metadata":{"id":"gsucnayJ7jmb"},"source":["# plot first few filters\n","n_filters, ix = 6, 1\n","for i in range(n_filters):\n","\t# get the filter\n","\tf = filters[:, :, :, i]\n","\t# plot each channel separately\n","\tfor j in range(3):\n","\t\t# specify subplot and turn of axis\n","\t\tax = plt.subplot(n_filters, 3, ix)\n","\t\tax.set_xticks([])\n","\t\tax.set_yticks([])\n","\t\t# plot filter channel in grayscale\n","\t\tplt.imshow(f[:, :, j], cmap='gray')\n","\t\tix += 1\n","# show the figure\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ib-M6v1cu96t"},"source":["**III. Visualization of specific features in a CNN**\n","<br><br>\n","The code example below produces the visualization of all the 64 feature maps for the first convolutional layer in the VGG16 model for a bird input image.\n","\n","We can see that the result of applying the filters in the first convolutional layer is a lot of versions of the bird image with different features highlighted. For example, some highlight lines, other focus on the background or the foreground."]},{"cell_type":"code","metadata":{"id":"wkz0J0Cm8yTd"},"source":["# plot feature map of first conv layer for given image\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.models import Model\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# load the model and redefine model to output right after the first hidden layer\n","model = VGG16() \n","model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n","model.summary()\n","\n","# load the image with the required shape and convert the image to an array\n","img = load_img('/content/gdrive/My Drive/DL/bird.jpg', target_size=(224, 224)) \n","img = img_to_array(img)\n","\n","# expand dimensions so that it represents a single 'sample'\n","img = np.expand_dims(img, axis=0)\n","\n","# prepare the image (e.g. scale pixel values for the vgg)\n","img = preprocess_input(img)\n","# get feature map for first hidden layer\n","feature_maps = model.predict(img)\n","print(feature_maps.shape)\n","\n","# plot all 64 maps in an 8x8 squares\n","square = 8\n","ix = 1\n","\n","for _ in range(square):\n","\tfor _ in range(square):\n","\t\t# specify subplot and turn of axis\n","\t\tax = plt.subplot(square, square, ix)\n","\t\tax.set_xticks([])\n","\t\tax.set_yticks([])\n","\t\t# plot filter channel in grayscale\n","\t\tplt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n","\t\tix += 1\n","# show the figure\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAOMj3KJIiea"},"source":["**Exercise**: Modify the program above to visualize the feature maps at the 2nd layer of conv block 2."]},{"cell_type":"markdown","metadata":{"id":"F7t3HQzPNk0P"},"source":["**Advanced Visualization**<br>\n","For more advanced visualization capabilities, you can check out **Grad-cam: Visual explanations from deep networks via gradient-based localization**: https://github.com/jacobgil/keras-grad-cam and a **Keras tutorial on Grad-cam**: https://keras.io/examples/vision/grad_cam/"]}]}