{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom re import sub\nimport tensorflow as tf\nfrom tensorflow.keras import metrics\n\nimport matplotlib.colors as mcolors\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Activation, Flatten, Dropout, Dense\nimport sklearn.metrics\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing import image\n\nfrom keras import backend as K\n\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import models\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\nprint(\"Done importing packages!\")","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:41:38.390032Z","iopub.execute_input":"2021-11-16T15:41:38.390347Z","iopub.status.idle":"2021-11-16T15:41:43.317878Z","shell.execute_reply.started":"2021-11-16T15:41:38.390268Z","shell.execute_reply":"2021-11-16T15:41:43.31701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nIMG_SIZE = (255, 255)\n\ndata_dir = \"../input/plantvillage-dataset/color\"\ntrain_dataset = image_dataset_from_directory(data_dir,\n                                             shuffle=True,\n                                             label_mode = 'categorical',\n                                             validation_split = 0.2,\n                                             batch_size=BATCH_SIZE,\n                                             seed = 42,\n                                             subset = \"training\",\n                                             image_size=IMG_SIZE)\n\nvalidation_dataset = image_dataset_from_directory(data_dir,\n                                             shuffle=True,\n                                             label_mode = 'categorical',\n                                             validation_split = 0.2,\n                                             batch_size=BATCH_SIZE,\n                                             seed = 42,\n                                             subset = \"validation\",\n                                             image_size=IMG_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:41:43.319203Z","iopub.execute_input":"2021-11-16T15:41:43.319524Z","iopub.status.idle":"2021-11-16T15:42:39.028126Z","shell.execute_reply.started":"2021-11-16T15:41:43.319495Z","shell.execute_reply":"2021-11-16T15:42:39.027065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = train_dataset.class_names\nnum_classes = len(class_names)\n\nfor i in range(1, num_classes + 1):\n    print(str(i) + \". \", class_names[i - 1])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:39.029834Z","iopub.execute_input":"2021-11-16T15:42:39.030331Z","iopub.status.idle":"2021-11-16T15:42:39.046661Z","shell.execute_reply.started":"2021-11-16T15:42:39.03029Z","shell.execute_reply":"2021-11-16T15:42:39.045913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\nprint('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:39.048801Z","iopub.execute_input":"2021-11-16T15:42:39.049057Z","iopub.status.idle":"2021-11-16T15:42:39.060037Z","shell.execute_reply.started":"2021-11-16T15:42:39.049033Z","shell.execute_reply":"2021-11-16T15:42:39.059207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:39.062108Z","iopub.execute_input":"2021-11-16T15:42:39.062573Z","iopub.status.idle":"2021-11-16T15:42:39.068674Z","shell.execute_reply.started":"2021-11-16T15:42:39.062537Z","shell.execute_reply":"2021-11-16T15:42:39.067716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add more augmentations\ndata_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:39.07011Z","iopub.execute_input":"2021-11-16T15:42:39.070783Z","iopub.status.idle":"2021-11-16T15:42:39.095448Z","shell.execute_reply.started":"2021-11-16T15:42:39.070747Z","shell.execute_reply":"2021-11-16T15:42:39.094722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\nMETRICS = [\n      metrics.CategoricalAccuracy(name='accuracy'),\n      precision_m,\n      recall_m,\n      f1_m\n]","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:39.096836Z","iopub.execute_input":"2021-11-16T15:42:39.097399Z","iopub.status.idle":"2021-11-16T15:42:39.114975Z","shell.execute_reply.started":"2021-11-16T15:42:39.097364Z","shell.execute_reply":"2021-11-16T15:42:39.114012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SHAPE = IMG_SIZE + (3,)\n# preprocess_input = tf.keras.applications.inception_v3.preprocess_input \n# preprocess_input = tf.keras.applications.resnet50.preprocess_input \npreprocess_input = tf.keras.applications.mobilenet.preprocess_input\n\n# base_model = tf.keras.applications.InceptionV3(\n#                                 include_top=False,\n#                                 weights=\"imagenet\",\n#                                 input_shape=IMG_SHAPE,\n#                             )\n\n# base_model =tf.keras.applications.resnet50.ResNet50(\n#                                 include_top=False,\n#                                 weights=\"imagenet\",\n#                                 input_shape=IMG_SHAPE,\n#                             )\n\nbase_model =tf.keras.applications.MobileNet(\n                                include_top=False,\n                                weights=\"imagenet\",\n                                input_shape=IMG_SHAPE,\n                            )","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:39.118711Z","iopub.execute_input":"2021-11-16T15:42:39.120402Z","iopub.status.idle":"2021-11-16T15:42:40.320951Z","shell.execute_reply.started":"2021-11-16T15:42:39.120375Z","shell.execute_reply":"2021-11-16T15:42:40.320101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.keras.utils.plot_model(\n#     base_model, show_shapes=True, show_dtype=True,\n#     show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n# )","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:40.322799Z","iopub.execute_input":"2021-11-16T15:42:40.323054Z","iopub.status.idle":"2021-11-16T15:42:40.326113Z","shell.execute_reply.started":"2021-11-16T15:42:40.323031Z","shell.execute_reply":"2021-11-16T15:42:40.325285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:40.327326Z","iopub.execute_input":"2021-11-16T15:42:40.327819Z","iopub.status.idle":"2021-11-16T15:42:40.340579Z","shell.execute_reply.started":"2021-11-16T15:42:40.327784Z","shell.execute_reply":"2021-11-16T15:42:40.339784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch, label_batch = next(iter(train_dataset))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:40.342992Z","iopub.execute_input":"2021-11-16T15:42:40.343481Z","iopub.status.idle":"2021-11-16T15:42:48.453342Z","shell.execute_reply.started":"2021-11-16T15:42:40.343447Z","shell.execute_reply":"2021-11-16T15:42:48.452334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\n\nprint(feature_batch_average.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:48.454859Z","iopub.execute_input":"2021-11-16T15:42:48.455205Z","iopub.status.idle":"2021-11-16T15:42:48.468325Z","shell.execute_reply.started":"2021-11-16T15:42:48.455164Z","shell.execute_reply":"2021-11-16T15:42:48.467297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:48.470021Z","iopub.execute_input":"2021-11-16T15:42:48.470593Z","iopub.status.idle":"2021-11-16T15:42:48.487238Z","shell.execute_reply.started":"2021-11-16T15:42:48.470554Z","shell.execute_reply":"2021-11-16T15:42:48.486324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inputs = tf.keras.Input(shape=(255, 255, 3))\n# x = data_augmentation(inputs)\n# x = preprocess_input(x)\n# x = base_model(x, training=True)\n# x = global_average_layer(x)\n# x = tf.keras.layers.Dropout(0.2)(x)\n# outputs = prediction_layer(x)\n# model = tf.keras.Model(inputs, outputs)\n\ninputs = tf.keras.Input(shape=(255, 255, 3))\n\nx = data_augmentation(inputs)\nx = preprocess_input(x)\n\nx = tf.keras.applications.InceptionV3(include_top=False, weights=\"imagenet\",input_shape=IMG_SHAPE)\n\nfor layer in x.layers:\n\tlayer.trainable = False\n    \nflat1 = global_average_layer(x.layers[-1].output)\nflat2 = tf.keras.layers.Dropout(0.2)(flat1)\nclass1 = Dense(1024, activation='relu')(flat2)\noutput = Dense(38, activation='softmax')(class1)\nmodel = tf.keras.Model(inputs=x.inputs, outputs=output)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:48.488449Z","iopub.execute_input":"2021-11-16T15:42:48.488933Z","iopub.status.idle":"2021-11-16T15:42:51.298345Z","shell.execute_reply.started":"2021-11-16T15:42:48.488899Z","shell.execute_reply":"2021-11-16T15:42:51.297463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_learning_rate = 0.001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n              metrics=METRICS)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:51.299735Z","iopub.execute_input":"2021-11-16T15:42:51.300082Z","iopub.status.idle":"2021-11-16T15:42:51.32026Z","shell.execute_reply.started":"2021-11-16T15:42:51.300046Z","shell.execute_reply":"2021-11-16T15:42:51.319327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:51.32175Z","iopub.execute_input":"2021-11-16T15:42:51.322139Z","iopub.status.idle":"2021-11-16T15:42:51.462039Z","shell.execute_reply.started":"2021-11-16T15:42:51.322097Z","shell.execute_reply":"2021-11-16T15:42:51.461171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\ncheckpoint_path = '/output/Checkpoint/'\nkeras_callbacks   = [\n      EarlyStopping(monitor='val_loss', patience=30, mode='min', min_delta=0.0001),\n      ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')\n]\ninitial_epochs = 5","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:51.463821Z","iopub.execute_input":"2021-11-16T15:42:51.464148Z","iopub.status.idle":"2021-11-16T15:42:51.469198Z","shell.execute_reply.started":"2021-11-16T15:42:51.464113Z","shell.execute_reply":"2021-11-16T15:42:51.468392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_epochs = 5","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:51.470559Z","iopub.execute_input":"2021-11-16T15:42:51.470897Z","iopub.status.idle":"2021-11-16T15:42:51.479189Z","shell.execute_reply.started":"2021-11-16T15:42:51.470863Z","shell.execute_reply":"2021-11-16T15:42:51.4784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(train_dataset,\n#                     epochs=initial_epochs,\n#                     validation_data=validation_dataset)\n\nhistory = model.fit(train_dataset,\n                    batch_size=32,\n                    epochs=initial_epochs,\n                    validation_data=validation_dataset,\n                    callbacks=keras_callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T15:42:51.4824Z","iopub.execute_input":"2021-11-16T15:42:51.482711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n\ndef GradCam(model, img_array, layer_name, eps=1e-8):\n    '''\n    Creates a grad-cam heatmap given a model and a layer name contained with that model\n    \n    Args:\n      model: tf model\n      img_array: (img_width x img_width) numpy array\n      layer_name: str\n\n    Returns \n      uint8 numpy array with shape (img_height, img_width)\n\n    '''\n    gradModel = Model(\n\t\t\tinputs=[model.inputs],\n\t\t\toutputs=[model.get_layer(layer_name).output,\n\t\t\t\tmodel.output])\n    \n    with tf.GradientTape() as tape:\n\t\t\t# cast the image tensor to a float-32 data type, pass the\n\t\t\t# image through the gradient model, and grab the loss\n\t\t\t# associated with the specific class index\n      inputs = tf.cast(img_array, tf.float32)\n      (convOutputs, predictions) = gradModel(inputs)\n      loss = predictions[:, 0]\n\t\t# use automatic differentiation to compute the gradients\n    grads = tape.gradient(loss, convOutputs)\n    \n    # compute the guided gradients\n    castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n    castGrads = tf.cast(grads > 0, \"float32\")\n    guidedGrads = castConvOutputs * castGrads * grads\n\t\t# the convolution and guided gradients have a batch dimension\n\t\t# (which we don't need) so let's grab the volume itself and\n\t\t# discard the batch\n    convOutputs = convOutputs[0]\n    guidedGrads = guidedGrads[0]\n    # compute the average of the gradient values, and using them\n\t\t# as weights, compute the ponderation of the filters with\n\t\t# respect to the weights\n    weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n    cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n  \n    # grab the spatial dimensions of the input image and resize\n\t\t# the output class activation map to match the input image\n\t\t# dimensions\n    (w, h) = (img_array.shape[2], img_array.shape[1])\n    heatmap = cv2.resize(cam.numpy(), (w, h))\n\t\t# normalize the heatmap such that all values lie in the range\n\t\t# [0, 1], scale the resulting values to the range [0, 255],\n\t\t# and then convert to an unsigned 8-bit integer\n    numer = heatmap - np.min(heatmap)\n    denom = (heatmap.max() - heatmap.min()) + eps\n    heatmap = numer / denom\n    # heatmap = (heatmap * 255).astype(\"uint8\")\n\t\t# return the resulting heatmap to the calling function\n    return heatmap\n\ndef sigmoid(x, a, b, c):\n    return c / (1 + np.exp(-a * (x-b)))\n\ndef superimpose(img_bgr, cam, thresh, emphasize=False):\n    \n    '''\n    Superimposes a grad-cam heatmap onto an image for model interpretation and visualization.\n    \n    Args:\n      image: (img_width x img_height x 3) numpy array\n      grad-cam heatmap: (img_width x img_width) numpy array\n      threshold: float\n      emphasize: boolean\n\n    Returns \n      uint8 numpy array with shape (img_height, img_width, 3)\n    '''\n    heatmap = cv2.resize(cam, (img_bgr.shape[1], img_bgr.shape[0]))\n    if emphasize:\n        heatmap = sigmoid(heatmap, 50, thresh, 1)\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    \n    hif = .8\n    superimposed_img = heatmap * hif + img_bgr\n    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n    superimposed_img_rgb = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n    return superimposed_img_rgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Grad-CAM heatmap for the last convolutional layer in the model, Conv_1\nimport cv2\n\nlayer_name = 'mixed10'\nimg = image_batch[15].numpy()\nimg = img.astype(np.uint8)\ngrad_cam=GradCam(model,np.expand_dims(img, axis=0),layer_name)\ngrad_cam_superimposed = superimpose(img, grad_cam, 0.5, emphasize=True)\n\nplt.figure(figsize=(12, 5))\nax = plt.subplot(1, 2, 1)\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('Original Image')\nax = plt.subplot(1, 2, 2)\nplt.imshow(grad_cam_superimposed)\nplt.axis('off')\nplt.title('mixed10 Grad-CAM heat-map')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv2D_layers = [layer.name for layer in reversed(base_model.layers) if len(layer.output_shape) == 4 and isinstance(layer, tf.keras.layers.Conv2D)]\nactivation_layers = [layer.name for layer in reversed(base_model.layers) if len(layer.output_shape) == 4 and layer.__class__.__name__ == 'ReLU']\nall_layers = [layer.name for layer in reversed(base_model.layers) if len(layer.output_shape) == 4 and (layer.__class__.__name__ == 'ReLU' or isinstance(layer, tf.keras.layers.Conv2D))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Visualizing Activation layers\nplt.figure(figsize=(15, 15))\nfor i, layer in enumerate(conv2D_layers):\n  grad_cam = GradCam(model, np.expand_dims(img, axis=0), layer)\n  grad_cam_emphasized = superimpose(img, grad_cam, 0.5, emphasize=False)\n  ax = plt.subplot(14, 5, i +1)\n  plt.imshow(grad_cam_emphasized)\n  plt.title(layer)\n  plt.axis(\"off\")\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model in JSON format\nmodel_json = model.to_json()\njson_file = open(\"../working/model_weigts.json\", \"w\")\njson_file.write(model_json)\nprint(\"Model saved in JSON format!\")\n    \n# save training weights in h5 file\nmodel.save_weights(\"../working/model_weigts.h5\")\nprint(\"\\nModel weights saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"inception_V3.0_fineTuning.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor layer in base_model.layers:\n    # check for convolutional layer\n    if 'conv' not in layer.name:\n        continue\n    # get filter weights\n    filters = layer.get_weights()\n    print(len(filters))\n# \tprint(layer.name)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) / (f_max - f_min)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot first few filters\nn_filters, ix = 6, 1\nfor i in range(n_filters):\n\t# get the filter\n\tf = filters[:, :, :, i]\n\t# plot each channel separately\n\tfor j in range(3):\n\t\t# specify subplot and turn of axis\n\t\tax = plt.subplot(n_filters, 3, ix)\n\t\tax.set_xticks([])\n\t\tax.set_yticks([])\n\t\t# plot filter channel in grayscale\n\t\tplt.imshow(f[:, :, j], cmap='gray')\n\t\t# print(f[:, :, j].shape)\n\t\tix += 1\n# show the figure\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}